# NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE

在本文中，我们推测使用固定长度向量是提高基本编码器-解码器架构性能的瓶颈，并建议通过允许模型自动(软)搜索源句子中与预测目标词相关的部分，而不必将这些部分明确地形成硬段来扩展这一点。



## Introduction

提出了编码器-解码器架构存在的问题：神经网络需要能够将源句子的所有必要信息压缩成固定长度的向量（个人理解：最后都会转化为[num_seq, batch_size, num_hidden]的向量，这样一来信息被压缩成为num_hidden长度的向量了）。这可能会使神经网络难以处理长句子，特别是那些比训练语料库中的句子更长的句子。

为了解决这个问题，我们引入了一种扩展的编码器-解码器模型（扩展？具体扩展在哪里？），该模型可以学习对齐和翻译。

每次提出的模型在翻译中生成一个单词时，它(软)搜索源句子中**最相关**信息集中的一组位置。然后，该模型根据与这些源位置和之前生成的所有目标单词相关的上下文向量来预测目标单词。